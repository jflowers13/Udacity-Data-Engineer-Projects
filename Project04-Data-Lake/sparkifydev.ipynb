{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### This notebook is used for developing and testing code for Sparkify in local mode before moving on to bigger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports from Python and Spark\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, \\\n",
    "    DoubleType as Dbl, StringType as Str, IntegerType as Int, LongType as LInt, TimestampType\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GET input and output paths\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "input_data = config['PATHS']['LOCAL_INPUT_DATA']\n",
    "output_data = config['PATHS']['LOCAL_OUTPUT_DATA']\n",
    "\n",
    "print(input_data)\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get or create Spark Session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read and Extract Song data\n",
    "\n",
    "# get filepath to song data files\n",
    "song_data = os.path.join(input_data, 'song_data/*/*/*/*.json')\n",
    "\n",
    "# Specify Schema for reading song data\n",
    "songdata_schema = R([\n",
    "    Fld(\"artist_id\", Str()),\n",
    "    Fld(\"artist_latitude\", Dbl()),\n",
    "    Fld(\"artist_location\", Str()),\n",
    "    Fld(\"artist_longitude\", Dbl()),\n",
    "    Fld(\"artist_name\", Str()),\n",
    "    Fld(\"duration\", Dbl()),\n",
    "    Fld(\"num_songs\", Int()),\n",
    "    Fld(\"song_id\", Str()),\n",
    "    Fld(\"title\", Str()),\n",
    "    Fld(\"year\", Int()),\n",
    "])\n",
    "\n",
    "# read song data\n",
    "df = spark.read.json(song_data, schema=songdata_schema, multiLine=True, mode='PERMISSIVE', columnNameOfCorruptRecord='corrupt_record')\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").dropDuplicates()\n",
    "\n",
    "songs_table.printSchema()\n",
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\", \"artist_id\").parquet(os.path.join(output_data, 'songs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read and Extract Artist data from song-data.zip\n",
    "\n",
    "# extract columns to create artists table\n",
    "artist_cols = [\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\"]\n",
    "\n",
    "artists_table = df.selectExpr(artist_cols).dropDuplicates()\n",
    "\n",
    "artists_table.printSchema()\n",
    "artists_table.show(5)\n",
    "artists_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'artists'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create view for use in creating songplays table in log processing function\n",
    "df.createOrReplaceTempView('song_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read and Extract user data from log-data.zip file\n",
    "\n",
    "# get filepath to log data file\n",
    "log_data = os.path.join(input_data, 'log_data/*.json')\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data, multiLine=True, mode='PERMISSIVE', columnNameOfCorruptRecord='corrupt_record')\n",
    "    \n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# extract columns for users table\n",
    "users_cols = [\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\"]\n",
    "\n",
    "users_table = df.selectExpr(users_cols).dropDuplicates()\n",
    "\n",
    "users_table.printSchema()\n",
    "users_table.show(5)\n",
    "users_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, 'users'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read and Extract time data from log-data.zip file\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: str(datetime.fromtimestamp(int(x) / 1000.0))) \n",
    "df = df.withColumn(\"start_time\", get_timestamp(col('ts')))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select(\"start_time\").dropDuplicates() \\\n",
    "    .withColumn(\"hour\", hour(col(\"start_time\").cast(TimestampType()))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"start_time\").cast(TimestampType()))) \\\n",
    "    .withColumn(\"week\", weekofyear(col(\"start_time\").cast(TimestampType()))) \\\n",
    "    .withColumn(\"month\", month(col(\"start_time\").cast(TimestampType()))) \\\n",
    "    .withColumn(\"year\", year(col(\"start_time\").cast(TimestampType()))) \\\n",
    "    .withColumn(\"weekday\", date_format(col(\"start_time\").cast(TimestampType()), 'E'))\n",
    "\n",
    "time_table.printSchema()\n",
    "time_table.show(22)\n",
    "time_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(os.path.join(output_data, 'time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Songplay Fact Table using song data and log data\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "# song_df = spark.read.parquet(os.path.join(output_data, 'songs'))\n",
    "song_df = song_df = spark.sql('SELECT DISTINCT song_id, title, artist_id, artist_name,duration FROM song_df_table')\n",
    "\n",
    "song_df.printSchema()\n",
    "song_df.show(5)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table partitioned by year and month\n",
    "songplays_table = df.join(song_df, (df.song == song_df.title) & (df.artist == song_df.artist_name) & (df.length == song_df.duration), how='left_outer') \\\n",
    "    .distinct() \\\n",
    "    .select(monotonically_increasing_id().alias(\"songplay_id\"),\n",
    "             col(\"start_time\"),\n",
    "             col(\"userId\").alias(\"user_id\"),\n",
    "             col(\"level\"),\n",
    "             col(\"song_id\"),\n",
    "             col(\"artist_id\"),\n",
    "             col(\"sessionId\").alias('session_id'),\n",
    "             col(\"location\"),\n",
    "             col(\"userAgent\").alias(\"user_agent\"),\n",
    "    ).withColumn(\"month\", month(col(\"start_time\"))) \\\n",
    "     .withColumn(\"year\", year(col(\"start_time\")))\n",
    "\n",
    "songplays_table.printSchema()\n",
    "songplays_table.show(92)\n",
    "songplays_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(os.path.join(output_data, 'songplays'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
